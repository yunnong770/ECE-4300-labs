{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 07: SVMs on Extended MNIST\n",
    "\n",
    "In the [MNIST demo](../Demos/demo_mnist_svm.ipynb), we saw how SVMs can be used for the classic MNIST problem of digit recognition. In this lab, we are going to extend the MNIST dataset by adding a number of non-digit letters and see if the classifier can distinguish the digits from the non-digits. All non-digits will be lumped as a single 11-th class. This is a highly simplified version of a 'detection' problem (as opposed to 'classification' problem). Detection is vital in OCR and related problems since the non useful characters must be rejected. \n",
    "\n",
    "In addition to the concepts in the demo, you will learn:\n",
    "* Combine multiple datasets\n",
    "* Select the SVM parameters (`C` and `gamma`) via cross-validation.\n",
    "* Use the `GridSearchCV` method to search for parameters with cross-validation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we download the standard packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn import linear_model, preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the EMNIST Dataset\n",
    "\n",
    "After creating the highly popular MNIST dataset, NIST created an extended version of the dataset that includes letters and digits.\n",
    "The extended datbase (called EMNIST) also has many more examples per class.  \n",
    "\n",
    "To download the data, go to the [EMNIST webpage](https://www.nist.gov/itl/iad/image-group/emnist-dataset).  Near the bottom, you will see a link for `MATLAB format dataset`.  If you click on this link, you will download a `zip` file with several datasets in it.  The total file is 726M, so it may take some time and diskspace to download.  Extract two files:\n",
    "* `emnist-digits.mat`:  This is a file of digits `0` to `9`, but with more examples per class.\n",
    "* `emnist-letters.mat`:  This is a file of letters `a/A` to `z/Z`.  The lower and upper case letters are grouped into the same class.\n",
    "\n",
    "Once you get these two files, you can save yourself the diskspace and remove all the other files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:  Download emnist-letters.mat and emnist-digits.mat to the working directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since MATLAB files are still widely-used, Python has excellent routines for loading MATLAB files.  The function below uses the `scipy.io` package to extract the relevant fields from the MATLAB file.  Specifically, the function extracts the training and test data from MATLAB file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "def load_emnist(file_path='emnist-digits.mat'):\n",
    "    \"\"\"\n",
    "    Loads training and test data with ntr and nts training and test samples\n",
    "    The `file_path` is the location of the `eminst-balanced.mat`.\n",
    "    \"\"\"    \n",
    "    \n",
    "    # Load the MATLAB file\n",
    "    mat = scipy.io.loadmat(file_path)\n",
    "    \n",
    "    # Get the training data\n",
    "    Xtr = mat['dataset'][0][0][0][0][0][0][:]\n",
    "    ntr = Xtr.shape[0]\n",
    "    ytr = mat['dataset'][0][0][0][0][0][1][:].reshape(ntr).astype(int)\n",
    "    \n",
    "    # Get the test data\n",
    "    Xts = mat['dataset'][0][0][1][0][0][0][:]\n",
    "    nts = Xts.shape[0]\n",
    "    yts = mat['dataset'][0][0][1][0][0][1][:].reshape(nts).astype(int)\n",
    "    \n",
    "    print(\"%d training samples, %d test samples loaded\" % (ntr, nts))\n",
    "\n",
    "    return [Xtr, Xts, ytr, yts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the function above to get all the digit images from the `emnist-digits.mat` file.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240000 training samples, 40000 test samples loaded\n"
     ]
    }
   ],
   "source": [
    "# TODO:  Load the digit data from emnist-digits.mat\n",
    "# Xtr_dig, Xts_dig, ytr_dig, yts_dig = ...\n",
    "Xtr_dig, Xts_dig, ytr_dig, yts_dig = load_emnist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see 240000 training samples and 40000 test samples.  Now use the same function to get the letter characters from `emnist-letters.mat`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124800 training samples, 20800 test samples loaded\n"
     ]
    }
   ],
   "source": [
    "# TODO:  Load the digit data from emnist-letters.mat\n",
    "# Xtr_let, Xts_let, ytr_let, yts_let = ...\n",
    "Xtr_let, Xts_let, ytr_let, yts_let = load_emnist('emnist-letters.mat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see 124800 training samples and 20800 test samples.  Next, will recreate the `plt_digit` function from the demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_digit(x,y=None):\n",
    "    nrow = 28\n",
    "    ncol = 28\n",
    "    xsq = x.reshape((nrow,ncol))\n",
    "    plt.imshow(xsq.T,  cmap='Greys_r')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])    \n",
    "    if y != None:\n",
    "        plt.title('%d' % y)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot 8 random samples from the digit training data.  You can use the `plt_digit` function above with `subplot` to create a nice display.  I suggest sizing your plots using the `plt.figure(figsize=(10,20))` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO:  Plot 8 random samples from the training data of the digits\n",
    "nplt = 8\n",
    "nsamp = Xtr_dig.shape[0]\n",
    "np.random.seed()\n",
    "Iperm = np.random.permutation(nsamp)\n",
    "\n",
    "# Plot the images using the subplot command\n",
    "for i in range(nplt):\n",
    "    ind = Iperm[i]\n",
    "    plt.subplot(1,nplt,i+1)\n",
    "    plt_digit(Xtr_dig[ind,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, plot 8 random samples from the letters training data.  You should see that the labels go from 0 to 25, corresponding to `a` to `z`.  Upper and lower case letters belong to the same class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO:  Plot 8 random samples from the training data of the letters\n",
    "nplt = 8\n",
    "nsamp = Xtr_let.shape[0]\n",
    "np.random.seed()\n",
    "Iperm = np.random.permutation(nsamp)\n",
    "\n",
    "# Plot the images using the subplot command\n",
    "for i in range(nplt):\n",
    "    ind = Iperm[i]\n",
    "    plt.subplot(1,nplt,i+1)\n",
    "    plt_digit(Xtr_let[ind,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Non-Digit Class\n",
    "\n",
    "SVM classifiers are very slow to train.  The training is particularly slow when there are a large number of classes.  To speed things up, we are going to put all of the letters in one class and merge that class with the digits data.  \n",
    "\n",
    "Before we begin, we will remove all the samples corresponding to `i/I`, `l/L` and `o/O`, since otherwise these letters would get confused with the digits `0` and `1`.  Create arrays `Xtr_let_rem` and `ytr_let_rem` from the data `Xtr_let` and `ytr_let`, where the samples `i` with `ytr_let[i] == 9, 12, 15` are removed.   Create `Xts_let_rem` and `yts_let_rem` similarly.\n",
    "\n",
    "If you are clever, you can do this without a for-loop via python broadcasting and `np.all(..., axis=1)` command.  But, if you like, you can use a for-loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_list = np.array([9,12,15])\n",
    "nplt = 8\n",
    "nsamp = Xtr_dig.shape[0]\n",
    "np.random.seed()\n",
    "Iperm = np.random.permutation(nsamp)\n",
    "\n",
    "# TODO:  Create arrays with labels 9, 12 and 15 removed\n",
    "# Xtr_let_rem, ytr_let_rem = ...\n",
    "# Xts_let_rem, yts_let_rem = ...\n",
    "\n",
    "index9 = np.where(ytr_let == remove_list[0])\n",
    "index12 = np.where(ytr_let == remove_list[1])\n",
    "index15 = np.where(ytr_let == remove_list[2])\n",
    "index_to_remove = np.append(index9, np.append(index12,index15))\n",
    "Xtr_let_rem = np.delete(Xtr_let,index_to_remove,0)\n",
    "ytr_let_rem = np.delete(ytr_let,index_to_remove)\n",
    "index9 = np.where(yts_let == remove_list[0])\n",
    "index12 = np.where(yts_let == remove_list[1])\n",
    "index15 = np.where(yts_let == remove_list[2])\n",
    "index_to_remove = np.append(index9, np.append(index12,index15))\n",
    "Xts_let_rem = np.delete(Xts_let,index_to_remove,0)\n",
    "yts_let_rem = np.delete(yts_let,index_to_remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To speed things up even further, we will use only a small subset of the training and test data.  Of course, we will not get great results with this small dataset.  But we can still illustrate the basic concepts.  \n",
    "\n",
    "Create training arrays `Xtr1_dig` and `ytr1_dig` by selecting 5000 random samples from `Xtr_dig` and `ytr_dig`.  Then create training arrays `Xtr1_let` and `ytr1_let` by selecting 1000 random samples from `Xtr_let_rem` and `ytr_let_rem`.  Similarly, create test arrays `Xts1_dig,Xts1_let,yts1_dig,yts1_let` with 5000 digits and 1000 letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of training and test digits and letters\n",
    "ntr_dig = 5000\n",
    "ntr_let = 1000\n",
    "nts_dig = 5000\n",
    "nts_let = 1000\n",
    "\n",
    "# TODO Create sub-sampled training and test data\n",
    "# Xtr1_dig, ytr1_dig = ...\n",
    "index = np.random.randint(Xtr_dig.shape[0], size = ntr_dig)\n",
    "Xtr1_dig = Xtr_dig[index,:]\n",
    "ytr1_dig = ytr_dig[index]\n",
    "# Xts1_dig, yts1_dig = ...\n",
    "index = np.random.randint(Xts_dig.shape[0], size = nts_dig)\n",
    "Xts1_dig = Xts_dig[index,:]\n",
    "yts1_dig = yts_dig[index]\n",
    "# Xtr1_let, ytr1_let = ...\n",
    "index = np.random.randint(Xtr_let.shape[0], size = ntr_let)\n",
    "Xtr1_let = Xtr_let[index,:]\n",
    "ytr1_let = ytr_let[index]\n",
    "# Xts1_let, yts1_let = ...\n",
    "index = np.random.randint(Xts_let.shape[0], size = nts_let)\n",
    "Xts1_let = Xts_let[index,:]\n",
    "yts1_let = yts_let[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will merge our digit and letter training arrays.\n",
    "* Create an array `Xtr` by stacking `Xtr1_dig`, `Xtr1_let`.  This should result in 6000 total samples.\n",
    "* Create a new label vector `ytr` where `ytr[i] = ytr1_dig[i]` for any digit sample and `ytr[i]=10` for any letter sample.  Thus, all the letters are lumped into the single class with label 10.\n",
    "\n",
    "Create test arrays `Xts` and `yts` similarly. \n",
    "\n",
    "You may wish to use the `np.hstack` and `np.vstack` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:  Create combined letter and digit training and test data\n",
    "# Xtr, ytr = ..\n",
    "Xtr = np.vstack((Xtr1_dig, Xtr1_let))\n",
    "ytr = np.append(ytr1_dig, 10*np.ones((ytr1_let.shape[0])))\n",
    "# Xts, yts = ...\n",
    "Xts = np.vstack((Xts1_dig, Xts1_let))\n",
    "yts = np.append(yts1_dig, 10*np.ones((yts1_let.shape[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features above use pixel values from 0 to 255.  Let's rescale the pixels to the interval from -1 to 1.  This will yield slightly better performance.  We'll put the scaled data into arrays `Xtr1` and `Xts1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:  Rescale the data to the interval from -1 to 1\n",
    "# Xtr1 = ...\n",
    "# Xts1 = ...\n",
    "Xtr1 = (Xtr - 255/2.0) / (255/2.0)\n",
    "Xts1 = (Xts - 255/2.0) / (255/2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The SVM classifier\n",
    "\n",
    "Let's first create an SVM classifer with penalty `C=2.8` and an `rbf` kernel of width `gamma=.0073`. We will reexamine these choices later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "# TODO:  Create a classifier: a support vector classifier\n",
    "# svc = ...\n",
    "svc = svm.SVC(probability=False, kernel=\"rbf\", C=2.8, gamma=.0073, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the classifier using the scaled training data.  Although SVMs are slow to train, we have kept the training set small, so the fitting should take less than a minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=2.8, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma=0.0073, kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO:  Fit the classifier on the training data. \n",
    "svc.fit(Xtr1,ytr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute and print the accuracy on the test data.  This too should take less than a minute.  You should get an accuracy of around 89%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.07894176,  0.02977241, ..., -0.10633174, -0.        ,\n",
       "        -0.        ],\n",
       "       [ 0.        ,  0.22423612,  0.        , ..., -0.        , -0.        ,\n",
       "        -0.        ],\n",
       "       [ 0.        ,  0.14463206,  0.11147969, ..., -0.        , -0.        ,\n",
       "        -0.19640753],\n",
       "       ...,\n",
       "       [ 0.        ,  0.02495295,  0.4714742 , ..., -0.04654989, -0.        ,\n",
       "        -0.        ],\n",
       "       [ 0.        ,  0.03333928,  0.36580958, ..., -0.10684198, -0.53412231,\n",
       "        -0.07872957],\n",
       "       [ 0.6013796 ,  0.3840591 ,  1.36748958, ..., -0.03950832, -0.41339437,\n",
       "        -0.        ]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO:  Measure error on the test data\n",
    "#yhat_ts = svc.predict(Xts1)\n",
    "#acc = np.mean(yhat_ts == yts)\n",
    "#print(\"The accuracy on the test data is \", acc)\n",
    "svc.dual_coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error rate is bit higher than what we would get with digits only.  This is because the \"letters\" class is extremely complex.  \n",
    "\n",
    "To see this, print and then plot the normalized confusion matrix.  You should see that the error rate on the \"letters\" class is much higher than the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'yhat_ts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-a2399b3639bc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_printoptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinewidth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m90\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mC\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m11\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m11\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mC\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myhat_ts\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mC_normalized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myhat_ts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'true'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mC_normalized\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msuppress_small\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'yhat_ts' is not defined"
     ]
    }
   ],
   "source": [
    "# TODO:  Print and plot the normalized confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#from sklearn.metrics import plot_confusion_matrix\n",
    "from scikitplot.metrics import plot_confusion_matrix\n",
    "np.set_printoptions(linewidth = 90)\n",
    "C = np.zeros((11,11))\n",
    "C = confusion_matrix(yts, yhat_ts) + C\n",
    "C_normalized = confusion_matrix(yts, yhat_ts, normalize = 'true')\n",
    "print(np.array_str(C_normalized, precision=4, suppress_small=True))\n",
    "plot_confusion_matrix(yts, yhat_ts,normalize = 'true',figsize = (10,10))\n",
    "#plot_confusion_matrix(svc, Xts1, yts,normalize = 'true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the following:\n",
    "* the fraction of digits that are mislabeled as letters  \n",
    "* the fraction of letters that are mislabeled as digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:  Print the above error rates\n",
    "fdl = sum(C[0:10,10])/sum(sum(C[0:10][0:10]))\n",
    "fld = sum(C_normalized[10][0:10])\n",
    "print(\"The fraction of digits that are mislabeled as letters:\", fdl)\n",
    "print(\"The fraction of letters that are mislabeled as digits:\", fld)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing gamma and C via Cross-Validation (Using For-Loops)\n",
    "\n",
    "Above, and in the demo, we used externally provided `gamma` and `C` values.  In general, these parameters should be carefully chosen.  We will now choose these parameters via cross validation.\n",
    "\n",
    "Below, we will try the value of `C` and `gamma` specified in the arrays `C_test` and `gam_test`.  For each combination of `C` and `gamma`, we fit the model on the training data and measure its accuracy on the test data.  Then we will select the `(C,gamma)` pair that results in the best accuracy.   \n",
    "\n",
    "Ideally, we would try many more values for each of the parameters, but this would take a long time.  So, we will try only 3 values for each.  Still, this will take several minutes to complete. \n",
    "\n",
    "For this lab, you may do the parameter search over `C` and `gamma` in one of two ways:\n",
    "* This section:  Use for-loops and manually search over the parameters.  This is more direct, and you can see and control exactly what is happening.\n",
    "* Next section:  Use the `GridSearchCV` method in the `sklearn` package.  This takes a little reading, but once you learn this method, it is more efficient to program.\n",
    "\n",
    "**You only need to submit the solution to one of the two sections.**  Pick whichever one you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_test = [0.1,1,10]\n",
    "gam_test = [0.001,0.01,0.1]\n",
    "\n",
    "nC = len(C_test)\n",
    "ngam = len(gam_test)\n",
    "acc = np.zeros((nC,ngam))\n",
    "\n",
    "# TODO:  Measure and print the accuracy for each C and gamma value.  Store the results in acc\n",
    "for i in range(nC):\n",
    "    for j in range(ngam):\n",
    "        svc = svm.SVC(probability=False, kernel=\"rbf\", C=C_test[i], gamma=gam_test[j], verbose=1)\n",
    "        svc.fit(Xtr1,ytr)\n",
    "        yhat_ts = svc.predict(Xts1)\n",
    "        acc[i][j] = np.mean(yhat_ts == yts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:  Print the accuracy matrix\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:  Print the maximum accuracy and the corresponding best C and gamma\n",
    "print(\"The maximum accuracy is:\",np.max(acc))\n",
    "print(\"The corresponding best C is:\", C_test[np.where(acc == np.max(acc))[0][0]])\n",
    "print(\"The corresponding best gamma is:\", gam_test[np.where(acc == np.max(acc))[1][0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Optimizing gamma and C via Cross-Validation (Using `GridSearchCV`)\n",
    "\n",
    "As discussed above, we could implement cross-validation of `C` and `gamma` using for-loops.  But `sklearn` has an excellent method called `GridSearchCV` that automates this process.  In this lab, both for-loops and `GridSearchCV` are pretty easy to implement.  But, for complex parameter searches, `GridSearchCV` becomes much easier.  For example, `GridSearchCV` supports parallelization, so that different parameters can be tested simultaneously.  Below, we will implement cross-validation using `GridSearchCV`.  \n",
    "\n",
    "**You do not have to do this section, if you did the previous section**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `GridSearchCV` method does a train-test split in addition to the parameter search.  In this case, we already have a train-test split, and so we will first combine the train and test data back into a single dataset to use with `GridSearchCV`.\n",
    "\n",
    "Create arrays `X` and `y` from `Xtr1`, `Xts1`, `ytr` and `yts`.  Use `np.vstack` and `np.hstack`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:  Create combined trained and test data X and y.\n",
    "# X = ...\n",
    "# y = ...\n",
    "X = np.vstack((Xtr1,Xts1))\n",
    "y = np.hstack((ytr, yts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally, `GridSearchCV` will do $K$-fold validation and automatically split the data into training and test in each fold.  But, in this case, we want it to perform only one fold with a specific train-test split.  To do this, we need to do the following:\n",
    "* Create a vector `test_fold` where `test_fold[i] = -1` for the samples `i` in the training data (this indicates that they should not be used as test data in any fold) and `test_fold[i] = 0` for the samples `i` in the test data (this indicates that they should be as test data in fold 0).\n",
    "* Call the method  `ps = sklearn.model_selection.PredefinedSplit(test_fold)` to create a predefined test split object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:  Create a pre-defined test split object \n",
    "# import sklearn.model_selection\n",
    "# test_fold = ...\n",
    "# ps = sklearn.model_selection.PredefinedSplit(test_fold)\n",
    "import sklearn.model_selection\n",
    "test_fold = np.append(-1*np.ones(6000),np.zeros(6000))\n",
    "ps = sklearn.model_selection.PredefinedSplit(test_fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, read about the `GridSearchCV` method to set up a classifier that includes searching over the parameter grid.  \n",
    "* For the `param_grid` parameter, you will want to create a dictionary to search over `C` and `gamma`.  You will also need to select the `kernel` parameter.\n",
    "* Set `cv = ps` to use the fixed train-test split.\n",
    "* Set `verbose=10` to monitor the progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:  Create a GridSearchCV classifier\n",
    "# clf = ...\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "svc = svm.SVC()\n",
    "parameters = {'kernel':('linear', 'rbf'), 'C':[0.1, 1, 10], 'gamma': [0.001,0.01,0.1]}\n",
    "clf = GridSearchCV(svc, parameters, cv = ps, verbose = 10, return_train_score = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the classifier using the `fit` method.  The fit method will now search over all the parameters. This will take about 30 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fit the classifier\n",
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the `best_score_` and `best_params_` attributes of the classifier to find the best score and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:  Print the best parameter and score of the classifier\n",
    "print(\"The best test score:\", clf.best_score_)\n",
    "print(\"The best parameter:\", clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can print the test and train score from the `cv_results_['mean_test_score']` and `cv_results_['mean_train_score']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:  Print the mean test score for each parameter value.\n",
    "import pandas as pd\n",
    "pd = pd.DataFrame(clf.cv_results_)\n",
    "print(pd[['mean_test_score', 'mean_train_score']])\n",
    "#print(pd['mean_test_score'])\n",
    "#print(pd['mean_train_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
